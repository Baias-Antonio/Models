{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS1+7IuF/RdFw2qmL/9dFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baias-Antonio/My_work/blob/main/testing_HibridModel_newdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-As3rCK46tK",
        "outputId": "b1abfa5d-17c1-4473-da05-a18c064f63f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of HubertWithLSTMForSpeechClassification were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'lstm_model.fc.bias', 'lstm_model.fc.weight', 'lstm_model.lstm.bias_hh_l0', 'lstm_model.lstm.bias_hh_l0_reverse', 'lstm_model.lstm.bias_ih_l0', 'lstm_model.lstm.bias_ih_l0_reverse', 'lstm_model.lstm.weight_hh_l0', 'lstm_model.lstm.weight_hh_l0_reverse', 'lstm_model.lstm.weight_ih_l0', 'lstm_model.lstm.weight_ih_l0_reverse']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of HubertWithLSTMForSpeechClassification were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'lstm_model.fc.bias', 'lstm_model.fc.weight', 'lstm_model.lstm.bias_hh_l0', 'lstm_model.lstm.bias_hh_l0_reverse', 'lstm_model.lstm.bias_ih_l0', 'lstm_model.lstm.bias_ih_l0_reverse', 'lstm_model.lstm.weight_hh_l0', 'lstm_model.lstm.weight_hh_l0_reverse', 'lstm_model.lstm.weight_ih_l0', 'lstm_model.lstm.weight_ih_l0_reverse']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Making Predictions on Unlabeled Data: 100%|██████████| 60/60 [05:59<00:00,  6.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 6, 4, 1, 2, 3, 3, 3, 2, 3, 3, 3, 4, 5, 5, 5, 4, 2, 4, 7, 4, 4, 7, 4, 5, 4, 7, 6, 7, 6, 7, 7, 6, 6, 7, 6, 6, 7, 6, 6, 7, 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import HubertForSequenceClassification, Wav2Vec2Processor\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "from transformers import HubertPreTrainedModel\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for bidirectional LSTM\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = torch.mean(out, dim=1)  # Use mean across the time dimension\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Define the unlabeled dataset class\n",
        "class UnlabeledAudioDataset(Dataset):\n",
        "    def __init__(self, file_paths, processor):\n",
        "        self.file_paths = file_paths\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "\n",
        "        # Utilize torchaudio to load the audio file and convert to mono\n",
        "        waveform, sample_rate = torchaudio.load(file_path, normalize=True)\n",
        "\n",
        "        # Ensure the audio is in mono\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Resample audio to the correct sampling rate (16000 Hz)\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "        # Check if waveform has non-zero length\n",
        "        if waveform.shape[1] > 0:\n",
        "            # Process the waveform using the Wav2Vec2 processor\n",
        "            speech = self.processor(waveform[0].numpy(), padding=\"max_length\", truncation=True, max_length=6*16000,\n",
        "                                    return_tensors=\"pt\", sampling_rate=16000).input_values.squeeze(0)\n",
        "\n",
        "            return speech\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_load_path = '/content/drive/My Drive/best_modeltransformerlstm0608.pth'\n",
        "class HubertWithLSTMForSpeechClassification(HubertPreTrainedModel):\n",
        "    def __init__(self, config, lstm_hidden_size=256, num_classes=8):\n",
        "        super().__init__(config)\n",
        "        self.hubert = HubertModel(config)\n",
        "        self.lstm_model = LSTMModel(config.hidden_size, lstm_hidden_size, num_layers=1, num_classes=num_classes)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.hubert(x)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        x = self.lstm_model(hidden_states)\n",
        "        return x\n",
        "\n",
        "# Load the pre-trained Hubert model for speech emotion recognition\n",
        "fine_tuned_model_with_lstm = HubertWithLSTMForSpeechClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"facebook/hubert-large-ls960-ft\",\n",
        "    config=config\n",
        ")\n",
        "# Load the saved model\n",
        "loaded_model = HubertWithLSTMForSpeechClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"facebook/hubert-large-ls960-ft\",\n",
        "    config=config\n",
        ")\n",
        "loaded_model.load_state_dict(torch.load(model_load_path))\n",
        "loaded_model.eval()\n",
        "\n",
        "# Define the directory containing unlabeled audio files\n",
        "unlabeled_audio_dir =  '/content/drive/My Drive/Audio_Speech_Actors_01-24/Actor_07'\n",
        "\n",
        "\n",
        "# Get paths to unlabeled audio files\n",
        "unlabeled_audio_paths = [os.path.join(unlabeled_audio_dir, file) for file in os.listdir(unlabeled_audio_dir) if file.endswith(\".wav\")]\n",
        "\n",
        "# Create dataset for unlabeled data\n",
        "unlabeled_dataset = UnlabeledAudioDataset(unlabeled_audio_paths, processor)\n",
        "\n",
        "# Make predictions on unlabeled data\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for inputs in tqdm(unlabeled_dataset, desc='Making Predictions on Unlabeled Data'):\n",
        "        outputs = loaded_model(inputs.unsqueeze(0))\n",
        "        _, predicted_label = torch.max(outputs, 1)\n",
        "        predictions.append(predicted_label.item())\n",
        "\n",
        "# Print or save predictions\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6stk3nJMRph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}